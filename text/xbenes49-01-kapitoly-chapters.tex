

\chapter{Introduction}
\label{chapter:introduction}


Our~body as same as everything surrounding us emits some radiation. The~dominant wavelengths
belong to the~infrared spectrum and our body senses it as a heat. If we pass its significance
for living creatures and the~fact that the~presence of the~right amount of infrared radiation
is essencial for all the~life as we know it, there is also a~lot of usage in the~industry or
generally -- technology.

Infrared waves are used in various devices. From nightvision devices, astronomical telescopes to personal
electronics (infraport, TV remote controller). This thesis focuses on the usage in PIR
sensors -- electronic devices that changes its output based on the amount of received infrared
radiation.

The PIR sensors are used around us a lot even though we might not know it. We all know the waving
of hands towards the~sensor in a~hallway so the light would turn on and we could tie our shoes,
or the~self-opening door in shopping malls or self-rotating door in banks. These mechanisms mostly
use PIR sensors.

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{img/automaticdoorway.jpg}
    \caption{PIR sensor: automatic doorway. \cite{automaticdoorway} \label{fig:automaticdoorway}}
  \end{center}
\end{figure}

PIR sensors offer even more than stating a presence of person. It is possible to~process sensor
output signal to~get more information about~sensed space - a~position of~person, a~number of~people.
Especially when more sensors are used.

The localization using PIR is still matter of intense research, a number of articles has been
written on it. This thesis suggests multisensor attitude and a usage of fuzzy logic to merge
sensor's outputs.






\chapter{State of the art}
\label{theory}


\section{Physics of radiation}

In the~modelled system equations calculating with~infrared radiation are being used -- to~understand
them properly it is necessary to describe what is a~radiation, where does it come from and how can
we measure it. 

As it was already mentioned in the~chapter \ref{chapter:introduction}, every object whose temperature
is higher than absolute zero emits an~electromagnetic radiation.
\begin{equation}
T_{obj}>0~K\equiv -273.15^{\circ}C
\end{equation}
It is caused by a~charged subatomical particles (electrons, protons) that are undergoing an~acceleration,
emitting an~energy in a~form of photon -- electromagnetic radiation.


\subsection*{Characteristics}
The~electromagnetic radiation have a~number of measurable characteristics. The~most significant ones
are {\it frequency} $f$ and {\it wavelength} $\lambda$. Due to the~constant speed of the radiation $v$
aka speed of light $c = v = 3\cdot10^{8}~m\cdot s^{-1}$ not dependent on the frequency, they are
propotional and mutually transferrable.
\begin{equation}
f=\frac{v}{\lambda}=\frac{c}{\lambda}=\frac{3\cdot10^{8}}{\lambda}
\end{equation}

Electromagnetic waves are being divided into categories according to their usage by the wavelength
$\lambda$. With increasing wavelength~$\lambda$ it is gamma, X-rays, ultraviolet (UV), visible light,
infrared (IR) and radio waves. This is called electromagnetic spectrum and it is shown in the image
\ref{fig:spectrum}.

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{img/spectrum.png}
\caption{Electromagnetic spectrum.\label{fig:spectrum}}
\end{center}    
\end{figure}

Another measurable characteristic is an~energy of the~radiation $Q$,$E$ or $W$. It is linearly dependend
on its frequency $f$ and can be computed using Planck constant $h=6.63\cdot10^{-34}~J\cdot s $.
\cite{NasaEMSpectrum}
\begin{equation}
W = h\cdot f
\end{equation}

The~power of the~radiation $\Phi$ is called {\it radiant power} or rather {\it radiant flux}. As a~regular
power it is energy per time, since the radiation is four-dimentional, partial derivations must be used.
\begin{equation}
\Phi = \frac{\partial W}{\partial t}
\end{equation}

The radiant power per unit surface is a~flux density. It is called either {\it radiant exitance} $M$
when emitting or {\it irradiance} $E$ when receiving.
\begin{subequations}
\begin{equation}
M = \frac{\partial \Phi_{emitted}}{\partial S_{sender}}
\end{equation}
\begin{equation}
E = \frac{\partial \Phi_{received}}{\partial S_{receiver}}
\end{equation}
\end{subequations}
The~Stefan-Boltzmann law defines irradiance of electromagnetic radiation as
\begin{equation}
I = \sigma \cdot T^4
\end{equation}
where $\sigma = 5.6704\cdot 10^{-8} Wm^{-2}K^{-4}$ is the~Stefan-Boltzmann constant and $T$ is a~thermodynamic
temperature.

Power per unit solid angle $I$ is called {\it radiant intensity}. With dividing by an~area of the~item
projected from certain direction we get an~amount of power emitted in that~direction called {\it radiance} $L$.
\begin{subequations}
\begin{equation}
I = \frac{\partial \Phi}{\partial \Omega}
\end{equation}
\begin{equation}
L = \frac{\partial I}{\partial S cos(\theta)}
\end{equation}
\end{subequations}
Other characteristics of radiation can be seen in the~table \ref{table:units}. \cite{iso800007} \cite{TemperatureMeasuring}

\begin{table}
\begin{tabular}{|c|c|c|l|} \hline
\textbf{Name}             & \textbf{Symbol} & \textbf{Unit}                 & \textbf{Definition}                             \\ \hline
Radiant flux        & $\Phi$          & $W$                           & Power transfered by a radiation.                \\ \hline
Radiant exitance    & $M$             & $W\cdot m^{-2}$               & Sent $W$ per sender's surface.                  \\ \hline 
Irradiance          & $E$             & $W\cdot m^{-2}$               & Received $W$ per receiver's surface.            \\ \hline
Radiant intensity   & $I$             & $W\cdot sr^{-1}$              & $W$ per unit solid angle.                       \\ \hline
Radiance            & $L$             & $W\cdot sr^{-1}\cdot m^{-2}$  & $I$ per sender's area projected to a direction. \\ \hline
\end{tabular}
\caption{Radiation characteristics.\label{table:units} \cite{TemperatureMeasuring}}
\end{table}



\newpage
\section{Temperature homeostasis}
The animal bodies require physical and chemical conditions in order to work properly (or at all). One of
the physical aspects is a temperature. There are generally three types of animals -- {\it ectotherms},
{\it endotherms} and {\it mesotherms}.

Ectotherms do not regulate its body temperature and rely on an external source, endotherms keeps it
constant independently on the environment, so called {\it homeothermy}\footnote{Homeothermy is an aspect
of homeostasis. It means keeping its inner body temperature within the preset limits.}. Mesotherm
strategy is then something in between.

Endotherm groups are birds and mammals, the most significant ectotherm group are reptiles. They compensate
it with basking in the sun. The thermal characteristics of these groups can be seen in the figure \ref{fig:thermoregulatory}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.4\textwidth]{img/thermoregulatory.png}
\caption{Thermal regulation graph.\label{fig:thermoregulatory}\cite{thermoregulatory}}
\end{center}    
\end{figure}

Human body temperature $T_{HB}$ varies in $\langle 36^{\circ}C; 38^{\circ}C \rangle$, in the hyperthermia
it can rise up to $40^{\circ}C$. The figure \ref{fig:bodywavelength} shows the radiation wavelength composition.
The peak wavelength (temperature $37^{\circ}C$ or $310.15~K$) can be calculated with the Wien's displacement law.
\begin{equation}
\lambda_{max}=\frac{b}{T}=\frac{2.8977729 \cdot 10^{-3}}{310.15} = 9.3431~\mu m
\end{equation}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.4\textwidth]{img/bodyradiation.png}
\caption{Human body radiation wavelength.\cite{BodyRadiation}\label{fig:bodywavelength}}
\end{center}    
\end{figure}





\newpage
\section{Radiation perception}

\subsection*{Radiation processed by organisms}
This~thesis describes a~particular way how to use the~infrared radiation. Very important beginning of such
work is always studying existing applications. Unforgettable one is a~nature -- how evolution enabled 
various organisms to~use it.

Many animals can process parts of electromagnetic spectrum. Eyes enables mammals, cephalopods and arthropods
to sense a visible light, some insects can even see a~part of UV. Additionally organisms including human often
have thermoreceptors in~their skin so they can get information about intensity of infrared radiation around them.


\paragraph{Visible light}
\label{subsection:eye}
PIR~sensor structure is obviously inspired by a~human eye. Human eyes can process radiation
$\lambda \in \langle 380~nm;760~nm \rangle$ called visible light, one of the~bands of an~electromagnetic spectrum.
Seeing means receiving a light from a~light source reflected by the~surface of an~observed object to our retina.
A~biological system composed of~light-sensitive cells {\it rods} and {\it cones} propagates the~information
through nerves to brain.

A~ray coming to an~eye is going through a~converging lens, which changes its~trajectory aiming to~the~retina,
in~the~best case to~the~most sensitive place with a~lot~of~the~rods and cons called {\it Fovea~centralis}.
\cite{LightEyeVision}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.3\textwidth]{img/eye.png}
\caption{Structure of an eye. \cite{Eye}\label{fig:eye}}
\end{center}    
\end{figure}


\paragraph{Heat}
The~infrared rays surrounds us during our whole life, we sense it as~heat. The~heat receptors called
{\it thermoreceptors} in our body are located on its~surface (in the~skin), but also in~organs.
The~structure of~a~skin is shown in the figure \ref{fig:skin}. 

There is a~difference between sensing a~visible light and an~infrared radiation. Visible light comes
mostly reflected from the surface, while the IR can originate only from the~primary source -- warm item.

Our skin contains two types of~thermoreceptors: sensing cold, colder than a~body temperature and
hot, hotter than a~body temperature. The~skin structure is shown in~the~figure \ref{fig:skin}.
This is already well described, on the~other hand the~evaluation center of~these receptors in~the~brain
and its~mechanisms is~not fully understanded yet and a~matter of current research. \cite{BodilySenses}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.45\textwidth]{img/skin.jpg}
\caption{Structure of a~skin. \cite{SkinStructure}\label{fig:skin}}
\end{center}    
\end{figure}

Some animals even use sensing the heat as~a~primary way how to~survive. Several groups of~snakes
(pythons, rattlesnakes, boas and others) use it when hunting warm-blooded animals (mouses, rats, rabbits etc.).
Blood-eating organisms (vampire bats, south-american heteroptera {\it Triatoma infestans})
have IR receptors to~look for a~vein under the skin.\cite{SnakeInfrared}


\paragraph{Discovery}
For the~first time, the~infrared electromagnetic waves were observed and named in 1800 by German-English
astronome sir Frederick Harschel. He dispersed light by a~prism and found out that the~temperature
of~the~light is growing with wavelength, the~red light had the~highest one.

When he measured the~temperature behind the red light, there was no~visible light on~the~table but
the~thermometer was showing even higher temperature moving beyond~the~red spectrum. Harshel
pronounced hypothesis that except the~visible light there must be also invisible one which we can~not
see. \cite{HerschelLife}

%\begin{figure}[h!]
%\begin{center}
%\includegraphics[width=0.4\textwidth]{img/herschel.jpg}
%\caption{Sir William Herschel discovering the~infrared radiation.\cite{HerschelLife}\label{fig:herschel}}
%\end{center}    
%\end{figure}

A~great coincidence is that he was an~astronome, he even discovered the~planet Uran, and it is his discovery,
the~IR waves, which now enables us to~explore and understand the~universe. \cite{NasaIrVideo}



\subsection*{Infrared radiation processed by machines}
\label{IRsensing}
PIR ({\it passive infrared}) sensor is an electronic device that scans electromagnetic
radiation at~wavelength $\lambda\in \langle 700~nm;2.5~mm \rangle$ aka frequency $f\in \langle 120~MHz;430~THz \rangle$. \cite{an2105}

\paragraph{Principles of PIR sensor}
There is a~number of approaches how to~construct such a~sensor. The~point is to~convert the electromagnetic
energy in~electric voltage and send it away via wire to~be processed by~hardware or~software.

First way how to do it is {\it a~bolometer}. It uses the fact that resistance of a~resistor is different
when changing a~temperature, as shown in the equation \ref{eq:bolo} for temperature difference $\Delta T$
and resistor with original resistance $R_0$ and new resistance $R_t$ and with temperature coefficient $\alpha$.
So with using the~same voltage it measures the~electric current and with the~Ohm law $R = \frac{U}{I}$
the~sensor computes instantaneus resistance.

\begin{equation}
\label{eq:bolo}
R_t = R_0 (1 + \alpha\Delta T)
\end{equation}

Another type is {\it a thermoelectric sensor} reacting to the different thermoelectric resistance of
exposed wire and comparative wire.

The~last is {\it a~pyroelectric detector}. The~principle is based on~electrostatic polarization,
changing during the~temperature change. \cite{DetectorsBook}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.25\textwidth]{img/pirscheme.png}
\caption{Single element pyroelectric detector.\cite{an2105}\label{fig:pir}}
\end{center}    
\end{figure}


\paragraph{Sensing of the infrared radiation}
The~structure of~PIR sensor is inspirated by structure of an~eye described in~subsection \ref{subsection:eye}.
An infrared ray incoming to the~sensor first goes through~{\it Fresnel lens} aiming it onto~a~pyroelectric sensor
as you can see in the~figure \ref{fig:fresnellens}. Then the~ray is transformed in an~electric voltage.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.3\textwidth]{img/fresnellens.png}
\caption{Fresnel lens.\label{fig:fresnellens}}
\end{center}
\end{figure}

This sensor is made for application in sensing of people. People emit radiation with characterics
in the figure \ref{fig:bodywavelength}. The signal is amplified and filtered by built-in mechanism,
described later in \ref{chapter:sensorHW}, to well discriminate their presence and absence.
An example is a scheme \ref{fig:roomsegments} of product {\it PIR STD} made by {\it B+B Sensors}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/roomsegments.png}
\caption{Sensing of PIR STD. \cite{PIROperationalManual}\label{fig:roomsegments}}
\end{center}    
\end{figure}



\newpage
\section{Pattern recognition}
The pattern recognition means processing of a signal and localization of predefined objects.
The form is dependent on the type of signal and the objects that we search. Generally we can say
there are five parts of recognition pipeline.

\subsection*{Sensing}
In the world of digital computers sensing means {\it sampling}, converting a continuous signal
into discrete samples. It is present if the signal is being processed online.

Through different signal types various technologies for sensing are used -- image, sound, temperature,
pressure, weight, smell etc. The sensing procedure in the case of heat signal is described in the section
\ref{IRsensing}.

There is a few things that we need to deal during sensing: noise, linearity, callibration, ageing.

\subsection*{Segmentation}
The signal is splitted into segments by the time axis that are being processed separately. They can even overlap.
Segmentation ensures fast processing saving memory and other resources.

\subsection*{Features extraction}
Features are quantitive expression of the input signal, they replace the signal in the following phases.
Its purpose is to reduce memory and computational complexity of the processing. Each segment of $N$ samples
is transformed to vector of $K$ features, the point is to reduce dimensions, $K << N$, but preserve
relevant characteristics. Choosing the right features is therefore key for the following classifier.

To have good results the~features should be discriminative (distinguish between classes), invariant to
the transformations (translation, rotation, scale, deformation etc.) and decorrelated - mutually independent.

Vast number of described ways how to create features exists -- {\it Principal Component Analysis} (PCA),
{\it Linear Discriminant Analysis} (LDA). They can be used generally, but there is also many special
application-dependent features. 

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/featureextraction.png}
\caption{Preprocessing pipeline.\label{fig:featureextraction}}
\end{center}
\end{figure}

Multiple thesis and articles were written on the topic of heat signal processing. For the feature extraction,
\cite{SinglePIR} suggests {\it Wavelet Transformation}, \cite{ChirpletSVM} uses {\it chirplet}-based features,
but also tests other feature-extraction methods: PCA, Expanded-Class LDA or fusion of PCA and LDA feature vectors,
\cite{BayesanClassifier} calculates with the signal itself.

\subsection*{Classification}
Before we will describe the classification phase, several terms must be defined: 
\begin{itemize}
\item {\it Detection} is a~classifying of presence of observed object or~characteristic.
\item {\it Identification} is an~assigning the~observed object to~one of~$N$ classes.
\item {\it Detection Error Tradeoff} is a~relation of~miss to~false alarm probability of~a~classifier.
The~goal is minimizing both with finding the~best settings of~classifier parameters.
\end{itemize}
This thesis performs a detection of~a~presence of~a~person. In~the~case of~positive detection identification
of~the~situation is made -- what was actually detected and whether it is a~person or more people etc.

Finding the~most suitable classifier for~the~task is fundamental, but consequent to the~feature extraction
method we use. At~the~end of~the~day, inputs of~all the~methods is a~vector of~features for~each segment.
There are linear and non-linear classifiers, separating the~hyperdimentional space into~segments. Then
the~segment is detected or identified if features vector geometrically lies in~the right segment. 

It is also possible to~perform some kind of transformation before classification if the~space is not separable
linearly. But other attitudes are also possible like algorithm {\it K-nearest neighbors}.

An~output of a~classifier can be either a~hard decision or some~kind of soft score, which can be later processed
by a~postprocessor -- used to merge data from~more classifiers or something else. A~classical linear classifier
is used in {\it Linear Regression} or {\it Support Vector Machine} (SVM). Each neuron of recently very popular
{\it Neural Networks} (NN) can also be represented with linear classifier.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.3\textwidth]{img/linear.png}
\caption{Linear classifier.\cite{LinearClassifier}\label{fig:linearclassifier}}
\end{center}
\end{figure}

\subsection*{Postprocessing}
During postprocessing different information than pattern are used, procedure is
connected to the concrete task. This parts often includes hard decision, if the
classifier's output is a soft score -- prices are taken into consideration,
the simpliest way is using treshold.




\chapter{Design Description}

The design of the whole project is split in two products - sensing device
and the processing and visualising program - a server. Sensor measures signal
sensing the observed space with connected PIR sensor and sends the data. The server
collects the data from the sensor/sensors and performs the recognition of 
people and fusion algorithms over the results, if multiple sensors are connected.

Results might be displayed in the visualization program. The data are being sent
over network. In the implementation, LAN is used, but it could be possible to
use the Internet and send the data to the remote visualizer.


\section{Sensor device}

Heart of the sensing device is a preprogrammed MCU with PIR connected. Such device
uses AD convertor to read the signal value from the sensor. The device may also
perform fixed-sized segmentation in order to increase the effective data speed with
sending more samples within one chunk, but with regard to preserving {\it real-time}
nature.

One of the great issues, that need to be solved, is determining the position of the sensor,
or even worse -- mutual position of multiple sensors when used. Unless it is
known, no fusion can be done. If the intention is to sense the door area and count
number of incoming/outgoing people, then a position of the door is required to be known
in advance.

Very elegant solution is suggested in \cite{GestureControl}. Three PIR sensors are
places in fixed mutual position on a board, as shown in the figure \ref{fig:3pir_geometry}.
This brings several advantages, not only it increases the sensing angle,
but also solves problems with position of sensors, which is given by the board construction.
Such a construction is unsuitable to use in rooms though.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.4\textwidth]{img/3pir_geometry.png}
\caption{Geometry for three PIR sensors.\label{fig:3pir_geometry}}
\end{center}
\end{figure}

More flexible solution is probably inserting the dimension of space measured by the sensors
and their position directly to the fusion app as parameters. Issue of this is the positioning
itself. On the other hand, this solution can be easily extended with more parameters:
position of doors that can persons come or leave, windows and heaters that can have
unwanted influence and many others.

The device uses a serial communication channel, that can be read by the server. To
increase effectivity of the communication, the MCU sends the data in fixed period
as the chunk of fixed number of samples. The right size of the chunk must be chosen
with regard to the effectivity, but keeping the reaction fast enough. Sampling period and
sending period could be changed in the MCU configuration using REST API if the MCU
is reachable in a network and keeps a running web server.


\section{Classification server}
\label{section:classification}
On the used communication medium is running application listening to the channel.
This server parses the chunks sent by the devices and performs the classification.


\subsection*{Artefact extraction and fuzzification}
The signal produced by the PIR has a specific nature, as it is shown in the appendix \label{appendix:PIRSignal}.
There are parts, that are constant, divided by abruptly changing edges, when there is a moving body. The
characteristics of the movement change character of the signal.

The signal first is segmented using {\it continuous wavelet transformation}
\footnote{Continuous wavelet transformation is a signal processing technique transforming a
signal to frequency domain. The process is quite similar to fourier tranformation, instead
of sinusoid it uses wavelet, a finite signal with energy $E=0$, and therefore reacts better
to abrupt changes.\cite{WaveletTour}}
with Morlet wavelet (\ref{eq:morlet}) and scale $s_{cwt} = 1.33846$, and the extrems are
detected comparing to the $N_{n} = 8$ neighbors on both sides.

% Morlet "mexican hat" wavelet.
\begin{equation}
\Psi(x) = e^{-\frac{x^2}{2}} cos(5x)
\label{eq:morlet}
\end{equation}


Detected extrems are used as the borders of the segments, that lay in between them. These numbers were
discovered experimentally minimizing the sum of the within-segment variances of the recorded data using
formula \ref{eq:cwtparamssearch}

% Experimental optimalization of parameters.
\begin{equation}
s_{cwt}, N_{n} = \text{argmin} ( \sum_{\sigma_i \in \sigma_{s,N}} \sigma_i )
\label{eq:cwtparamssearch}
\end{equation}


In the next step segment distances are evaluated. Distance of two adjoining segments aka {\it edge} is computed as numerical
difference of their within-segment mean values.

% Distance of segments or edge height.
\begin{equation}
||\textbf{AB}|| = \mu_\textbf{B} - \mu_\textbf{A}
\end{equation}


Here comes to the game fuzzification. A designed set of fuzzy membership functions $\xi_F$, $\xi_S$ and $\xi_R$
corresponds with artefacts falling edge (\textbf{F}), stagnating signal (\textbf{S}) and rising edge (\textbf{R}).
The E and R are related, they use the same wave form based on transformed $arctan()$ function, parametrizable with
tolerance $t$ and center/shift $x_0$. S is based on gaussian function. The fuzzy set arrange is visualized
in the figure \ref{fig:fuzzysets} with $t=10$.

% Fuzzy membership functions.
\begin{subequations}
\begin{equation}
\xi_R(x) = \frac{ arctan( 4\cdot (\frac{x - x_0}{t} + 1) )}{\pi} + 0.5
\end{equation}
\begin{equation}
\xi_F(x) = \xi_R(-x)
\end{equation}
\begin{equation}
\xi_R(x) = \sqrt{ exp( -\frac{(x_0 - x)^2}{2\cdot (0.6t)^2} ) }
\end{equation}
\end{subequations}


\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.5\textwidth]{render/fuzzysets.png}
\caption{Fuzzy sets used for fuzzification.\label{fig:fuzzysets}}
\end{center}
\end{figure}

Every edge gets assigned its fuzzy value for each of the classes. This leads to determining {\it artefacts},
groups of neighboring segments, which has similar characteristic (falling, rising, stagnating), but it
changes in time and CWT separated them as isolated segments.

Score of border $\xi_{ABC}(x_i)$ for the edge $x_i$ is evaluated using formula \ref{fig:artefactborder}.
\footnote{Operator $\land_P$ here means {\it product fuzzy T-norm}: $A \land_P B = f_A(x)\cdot f_B(x)$.}
The calculation is done for all the artefact combinations $ABC$ here, where $A$ is characteristic of 
precedent $x_{i-1}$, $B$ for the $x_i$ itself and $C$ for the follower $x_{i+1}$. $A,B,C \in \{F,S,B\}$.

% Artefact combination fuzzy formula.
\begin{equation}
\xi_{ABC}(x_i) = (\xi_{A}(x_{i-1})) \land_P (\xi_{B}(x_i)) \land_P (\xi_{C}(x_{i+1}))
\label{fig:artefactborder}
\end{equation}

The combination indicating artefact border are put together as same as the rest. That is reached
using formula \ref{eq:edgemerger}.\footnote{Operator $\lor_{p\Sigma}$ here means
{\it probability sum fuzzy S-norm/T-conorm}: $A \lor_{p\Sigma} B = f_A(x) + f_B(x) - f_A(x)f_B(x)$.
Other variants could be considered too - maximum, Lukasiewicz, etc. }

Border indicating combinations are those, whose precedent and own combination differ, aka.
{\it \{SF*, FS*, RF*, FR*, RS*, SR*\} }. The rest, {\it \{FF*, SS*, RR*\}}, means no edge.
The results are tresholded for $T = 0.5$.

% Artefact border fuzzy formula.
\begin{subequations}
\begin{equation}
f_{1}[i] = \bigvee\limits_{\xi \in \xi_1}{}_{p\Sigma} (\xi)
\end{equation}
\begin{equation}
f_{0}[i] = \bigvee\limits_{\xi \in \xi_0}{}_{p\Sigma} (\xi)
\end{equation}
\label{eq:edgemerger}
\end{subequations}


Newly created artefacts will be used to form feature vectors, filled with each one's characteristics.
Suggested metrics are variance and mean value of samples within artefact, linear extrapolation scope
of the artefact, linear extrapolation scope of predecessor etc.

The linear extrapolation scope can be found by formula \ref{eq:artefactline}, where $l_0$, $l_1$ are
$y$ coordinates of first and last point of the line respectively, \textbf{A} is a artefact samples vector.
$k$ is the searched line scope, $\Delta_{\textbf{A},k}$ is the difference score of artefact \textbf{A} and
line with scope $k$. It can also be used as a feature. Graphical representation is to be seen in the figure
\ref{fig:siglines}.

\begin{subequations}
\begin{equation}
l[i] = k\textbf{A}[i] + l_0
\end{equation}
\begin{equation}
k = \frac{l_1 - l_0}{|\textbf{A}|}
\end{equation}
\begin{equation}
\Delta _{\textbf{A},k} = \sum_{i \in \{1,...,|\textbf{A}|\}} (\textbf{A}[i]-l[i])^2
\end{equation}
\begin{equation}
l_0,l_1 = \text{argmin} ( \Delta_{\textbf{A},k} )
\end{equation}
\label{eq:artefactline}
\end{subequations}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.85\textwidth]{render/signallines.png}
\caption{Representation of artefacts using line scope.\label{fig:siglines}}
\end{center}
\end{figure}

\subsection*{Classification}
Extracted feature vectors for every detected artefact will undergo classification procedure
consisting of multiple dichotomic classifiers, each evaluating single decision - whether
the movement is present or not, if the movement is in the center or on the aside, if the
movement is closer to sensor or far, and if possible, if the person is on the left side
or right.

The suggested solution of classification is based on linear regression, searching for the best linear
separation of the training data using linear classifier \ref{eq:linearclassifier}, where 
$\overline{x}$ is an input, $\sigma(a)$ is activation function mapping
$\mathbb{R} \rightarrow \langle 0;1\rangle$ and $\overline{w}$,$w_0$ are searched parameters
of the classifier.

\begin{equation}
y(x) = \sigma (\overline{w}^T \overline{x} + w_0)
\label{eq:linearclassifier}
\end{equation}

The estimation of classifier's parameters $\overline{w}$,$w_0$ means maximizing the
posterior probabilities of each training data as same as solving the equation
\ref{eq:regressionEq}, in the logarithm domain \ref{eq:regressionLog}.

\begin{subequations}
\begin{equation}
p(t|\textbf{X}) = argmax \prod_{n \in C_1}y(\overline{x}_n) \prod_{n \in C_2} \{ 1 - y( \overline{x}_n ) \}
\label{eq:regressionEq}
\end{equation}
\begin{equation}
-\text{ln}( p(t|\textbf{X}) ) = - \sum_{n=1}^N \{ (t_n)\text{ln}(y_n) + (1-t_n)\text{ln}(1-y_n) \} = E(\overline{w})
\label{eq:regressionLog}
\end{equation}
\end{subequations}

The searched extrem of \ref{eq:regressionLog} means derivation \ref{eq:regressionGradient}
equal to $0$, solved numerically using \ref{eq:regressionGradientNumerical}.
$\eta \in (0;1\rangle$ is a converging parameter, the computation can be stopped either after
fixed step count or using minimal solution improvement $\varepsilon < | w^{(\tau + 1)} - w^{(\tau)} |$.

\begin{subequations}
\begin{equation}
\nabla E(\overline{w}) = \sum_{n=1}^N (y_n - t_n)x_n = 0
\label{eq:regressionGradient}
\end{equation}
\begin{equation}
w^{(\tau + 1)} = w^{(\tau)} - \eta \nabla E(\overline{w})
\label{eq:regressionGradientNumerical}
\end{equation}
\end{subequations}

The classifier's output is a 2D fuzzy value matrix, representing the area in front of the sensor.
It is inspired by the space representation by cellular automata, which carves it into small
homogenous segments. The fuzzy value at certain index expresses a presence of a person
at the position. Even though the space because of the construction of sensor has a shape of
circular sector, it can be easily represented by classical 2D matrix, as it is shown in the figure \ref{fig:circularsector}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.5\textwidth]{img/circularsector_transformation.png}
\caption{Representation of circular sector.\label{fig:circularsector}}
\end{center}
\end{figure}

Each of the scores is merged together, creating the resulting matrix, as shown
in the figure \ref{fig:areamatrixweighting}. Formula \ref{eq:regressionmerger} illustrates
the base of merging process. The trait "object is closer than $T$" is scored as 
direct output of the classifier for the distance trait, for "object is further than T",
the score of the assertion is a fuzzy negation of the classifier output.
Classifier for $\kappa = distance$ must be trained for classes with distances $<T$,$>T$.

In this case, the area has only two distance states -- closer or more distant than T.
These traits are directly dependent on the labeling. With a sufficient number of training
data, a lot of traits can be created, enlarging the matrix and making the results more accurate.

For the assertions "object is on the left" and "object is on the right", a classifier trained
with appropriate labels is used, and the logic of evaluation is the same. Merging statements
is reaching the index score, and is designed using fuzzy T-norm of traits.


\begin{subequations}
\begin{equation}
M_{<T} = A^{\kappa = distance}
\end{equation}
\begin{equation}
M_{>T} = N(A^{\kappa = distance})
\end{equation}
\begin{equation}
M_{<0^{\circ}} = A^{\kappa = left}
\end{equation}
\begin{equation}
M_{>0^{\circ}} = N(A^{\kappa = left})
\end{equation}
\begin{equation}
M_{<0^{\circ}, <T~M} = N(A^{\kappa = left}) \land_P A^{\kappa = distance}
\end{equation}
\label{eq:regressionmerger}
\end{subequations}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.3\textwidth]{img/pir_area.png}
\caption{Weighting of regression score to area matrix.\label{fig:areamatrixweighting}}
\end{center}
\end{figure}


\subsection*{Defuzzification}

To get the results in a form of coordinate(s) of classified objects a cluster analysis is done. It calculates
a clusters of high membership values, for optimalization $\alpha$-cut on a certain level or tresholded
matrix can be used. Algorithm PAM (Partitioning Around Medoids is used), similar to k-means, where item called
medoid is used to represent the cluster instead of mean.

\begin{equation}
\mathit{PAM}(k, data) = argmin \left( \sum_{i=1}^{k} \sum_{j=1}^{k} ||data_{i} data_{j}||  \right)
\end{equation}

%\begin{lstlisting}[style=python]
%def PAM(k,data):
%  # pick medoids
%  medoids = [].generate(k,data.random())
%  # compute dissimilarity matrix
%  dm = DissimilarityMatrix(data,calculateDistance)
%  # create clusters
%  changed = True
%  while changed:
%    changed = False
%    clusters = []
%    for medoid_idx,medoid in medoids.enum():
%      cluster = []
%      for d in data:
%        # add to cluster with closest distance to its medoid
%        if dm[d,medoid] == dm[d,medoids].min():
%          cluster.append(d)
%      # set new center (SWAP phase)
%      cluster.append(medoid)
%      if medoid is not cluster.center():
%        medoids[medoid_idx] = cluster.center()
%        changed = True
%      clusters.append(cluster)
%  return clusters
%\end{lstlisting}

%\begin{lstlisting}[style=python]
%function elbow(data):
%  # try all k
%  best = inf
%  for k in <1,K_MAX>:
%    clusters = PAM(data, k)
%    # take minimal
%    if WCSS(clusters) < best:
%      best = clusters
%  return best
%\end{lstlisting}

To estimate the $k$, elbow method can be used: calculating k-means for different k values and taking the one with
minimal within-cluster sum of square (WCSS).\cite{ClusterAnalysis} \cite{VagueNatureInformation}

\begin{equation}
\mathit{WCSS} = \sum_{i=1}^{k} \sum_{x \in S_i} ||x - \mu_i||^2
\end{equation}

Very important is computing a distance of two segments. Since the original circular sector segmentation
is not homogenous, the distances varies with each distance. It can be calculated though using
cosine law. $P = (d, \alpha)$ is a segment given by polar coordinates, $d$ is distance and $\alpha$ is
azimuth.

\begin{equation}
|P_{1} P_{2}| = \sqrt{(d_{1})^{2} + (d_{2})^{2} - 2d_{1}d_{2}(\alpha_1 - \alpha_2)}
\end{equation}

\subsection*{Fusion}
It could be possible to use multiple sensors. Using more sensors brings advantages: the measuring
might be more precise since the person presence classified by two independent sensors is not only
more probable but the computed position can get more accurate than when using only one sensor.

The disadvantage is higher price and need for mechanism to process the configuration: mutual
orientation and position of the sensors, as same as position of the objects in the room
like doors or windows as "human sources", or borders of the room.

Fusion precedes defuzzification. All the computed sensor areas represented by fuzzy matrices
are merged into emptily-preinitialized whole area representation. Since these relations between
the indices are stacionary, it can be precounted after the setting the configuration for the room.

As shown in the figure \ref{fig:area2triangles}, the resulting matrix with fuzzy values
creates fragments with shape of either a circular sector of a circle, or a circular sector
of a annulus. To avoid overcomplicated computations, these shapes can be simplified by
representing them with triangles.

We have a circular sector with center $S$, radius $r$, arc edge points $L,R$ and their
polar angular deviations from the center are being $\varphi_L, \varphi_R$, global azimuth
of the sensor is $\omega$. Such a circular sector can be represented with a triangle $SLR$,
as shown in the formula \ref{eq:triangleCircularSector}.

Such object is in the figure \ref{fig:area2triangles} displayed split by its height.
Height can be easily found, it is a vector $SH$, where $H = L + \frac{1}{2}\overline{LR}$.

\begin{subequations}
\begin{equation}
S = S
\end{equation}
\begin{equation}
L = S + r*(cos(\omega + \varphi_L), sin(\omega + \varphi_L))
\end{equation}
\begin{equation}
R = S + r*(cos(\omega + \varphi_R), sin(\omega + \varphi_R))
\end{equation}
\label{eq:triangleCircularSector}
\end{subequations}

In the case of the circular sector of annulus, we have center $S$, polar angular deviations
$\varphi_L$,$\varphi_R$ and radiuses of the bordering arcs, minimal radius $r_1$ and 
maximal radius $r_2$. This can be overlaid by trapezoid $L_1,R_1,R_2,L_2$, the
$\Delta\varphi_L\varphi_R$ should not be to big, otherwise a significant discrepancies can occur. 
The formulas for stating the trapezoid points are shown in the formula \ref{eq:trapezoidCircularSector}.

\begin{subequations}
\begin{equation}
L_1 = S + r_1*(cos(\omega + \varphi_L), sin(\omega + \varphi_L))
\end{equation}
\begin{equation}
R_1 = S + r_1*(cos(\omega + \varphi_R), sin(\omega + \varphi_R))
\end{equation}
\begin{equation}
L_2 = S + r_2*(cos(\omega + \varphi_L), sin(\omega + \varphi_L))
\end{equation}
\begin{equation}
R_2 = S + r_2*(cos(\omega + \varphi_R), sin(\omega + \varphi_R))
\end{equation}
\label{eq:trapezoidCircularSector}
\end{subequations}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.4\textwidth]{img/area_triangles.png}
\caption{Sensed area converted to triangles.\label{fig:area2triangles}}
\end{center}
\end{figure}

All the objects were transformed into triangles. Now we get back to the global
area matrix representation. Below is described how to compute
size of area intersection of two triangles. If the global area is split into
homogenous mosaic of triangle sectors, the ones intersecting with sensor area
ones can be found in advance, just once and saved as a part of configuration.

The ratio of the intersecting sectors of sensor output matrixes over the
global area sector gives the coefficient of their influence in the fusion.
The not-covered parts of sector are considered as absence.

\paragraph{Two triangles intersection}
In order to count an intersection of two triangles, we will need formulas \ref{eq:2triangles}.

Let's consider two triangles $\Delta ABC$ and $\Delta DEF$, whose intersection is needed
to be found. Firstly for each of the edges of $\Delta ABC$, $AB$, $BC$ and $CA$,
collisions with points $D$, $E$, $F$ using $lp_{collide}$ and edges $DE$, $EF$, $FD$
with $ll_{collide}$ are evaluated and separately sorted for each "hit" by ascending
parameter $p$.

After that each of the vertices $D$, $E$, $F$ are tested for laying inside the triangle
(not on the edge though). If so, they are marked. Finally all these three lists are
merged together with vertices, if they are marked, in clockwise order, aka $DE$, $D^{*}$,
$EF$, $F^{*}$, $FD$, $D^{*}$.

\begin{subequations}

\begin{equation}
L_{A,B,p \in \langle 0;1)}(X) = A + p\cdot\overline{AB} - X
\end{equation}

\begin{equation}
lp_{collide}(A,B,C) = \begin{cases} L_{A,B,p}(X)       & L_{A,B,p} = 0 \\
                                    \text{not~on~line} & \text{otherwise}
                        \end{cases}
\end{equation}

\begin{equation}
ll_{collide}(A,B,C,D) = \begin{cases} X                       & lp_{A,B,X} \land L_{C,D,X} \\
                                      \text{no~intersection}  & \text{otherwise}
                        \end{cases}
\end{equation}

\label{eq:2triangles}
\end{subequations}

What needs to be point out, the object whose vertices were just counted is always convex.
Provided this is acknowledged the area of the object can be counted using altered 
Pined filling algorithm or another filling algorithm as same as for a triangle.





\chapter{Data}

A output of a sensor as you can see in the figures \ref{fig:signalcalm} and \ref{fig:signalwalk}
is very discriminative -- with a bare eye a movement from no movement is distinguishable.
A detection of presence used in light sensors can be implemented with tresholding, this attitude is
not very suitable for classification of anything else except the presence itself.

In a calm state the sensor sends a constant signal\footnote{Constant signal in the terms
of electricity, slightly polluted with a background noise etc.}. Movement in the sensed area causes
abrupt changes of output. When the object either leaves the area, or stays completely calm, the signal
changes are getting slower and after a little while the output voltage gets in the calm state again.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.65\textwidth]{render/signal_calm.png}
\caption{Signal of zero movement.\label{fig:signalcalm}}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.65\textwidth]{render/signal_walk.png}
\caption{Signal of person walking around.\label{fig:signalwalk}}
\end{center}
\end{figure}

Therefore, the nature of the signal does not seem to need a complicated method to perform a classification on it.
{\it Fourier transformation} and {\it wavelet transformation} were considered for feature
extraction. The abrupt changes in the signal could be problem for FT, because it can not represent
it efficiently\cite{SinglePIR}, % youtube video
but unlike sinusoids, the wavelets exist for a finite duration and they are suitable for representing
abrupt changes.

The wavelet transformation is defined as a function $F_{(s,k)}(x)$. Parameters s,k are scale and shift, changing
them in predefined unit and interval creates a matrix, as you can see in the figure \ref{fig:walk03}.

\begin{equation}
F_{(s,k)}(x) = \sum_{n=1}^{N} x[n] \Psi_{(s,k)}^{*}[n]
\end{equation}

Firstly The data were offline analysed using {\it Matlab}, which has implemented wavelet transformation.
The Matlab is not used because it is proprietary software and the program would be dependent on it.
During the analysis it was found, that the data are very well separable using continuous wavelet tranformation.

\begin{figure}[h!]
\begin{lstlisting}[style=matlab]
data = csvread('walk02.csv');
wscalogram('image', cwt(data));
\end{lstlisting}
\caption{Matlab code performing cwt.\label{list:cwtmatlab}}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.5\textwidth]{img/walk03.png}
\caption{Matlab {\it cwt()} output of {\it walk03.csv}.\label{fig:walk03}}
\end{center}
\end{figure}


The training and testing data were recorded as a part of the work, results are
shown and described by the appendix \ref{appendix:PIRSignal}.

\subsection*{Labeling the data set}
Classification is implemented as supervised learning. The supervisory is implemented
as labels for recorded files. For achieving such labels a special script {\it classifiers/labeller.py}
was made. This script uses {\it segment.py} of Monitor app to make the labeling possible.

The labeling itself is made by human. We have a recorded file with signal in the format .csv
and video file of the area as well in the format *.mp4. During the labeling, firstly the 
signal is segmented ino artefacts using classes Segment, Edge and Artefact. After that
artefacts are shown to the person performing the labeling.

When the program is started, it requires entering a session name to label. After that
measurements of the session are being processed. In order to work properly, if the
session is named X, the session measurements must be called X\_1.csv, X\_2.csv, ... .

Each artefact is shown as a signal plot, center frame of the video and the questions,
that the person ought to answer. The first question is "Is the person present?".
Clicking 0 or 1 on the keyboard with active window with a frame the program shows
another frame or question dependent on the answer. The consequent questions 
in the case of presence of the person are "Is the person in the center?", "Is the
parson on the left?" and "Is the person close?". After answering the next artefact
is processed.

The labeling process is shown in the figure \ref{fig:labeling}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.8\textwidth]{img/labeling2.png}
\caption{Labeling method of the data set.\label{fig:labeling}}
\end{center}
\end{figure}




\chapter{Implementation}

The implementation was done in two parts -- implementation of sensor device, that senses
the signal and sends it in chunks, described in \ref{chapter:sensorHW}, and the server,
that reads the data sent by sensor device and performs the classification, described in
\ref{chapter:serverImplementation}.

\section{Sensor device}
\label{chapter:sensorHW}

The sensor device consists of PIR sensor and programmed MCU that samples analog signal from
sensor and sends it via WiFi to a local multicast group.

Nowadays most of the PIR sensors available on market have only a binary {\it switching} output.
When signal reaches a set treshold, output is set to logic "1" for a unit of time.
This mechanism is suitable for a light sensor or door sonsor, completely useless
for the needs of this project though.

The only found sensor that offers an analog output was {\it PIR STD} by {\it B+B Sensors}.

\subsection*{PIR STD}
{\it PIR STD} is a product of {\it B+B Sensors}. It is the only found passive infrared
sensor, that has except of the switching binary ouput also analog output. Except of
operating voltage $V_{cc}$ and ground $GND$ pins, it also has reference voltage input,
which needs to be connected to $\frac{V_{cc}}{2} V$. The optical resistance pins can
be also used for additional classification, fusion with the infrared signal classification
results and making the final result more accurate.

Pin layout table from the sensor operational manual is shown in the table \ref{fig:pirstdpin}.

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c c | c | c |} \hline
\textbf{Pin} & \textbf{Code} & \textbf{Type} & \textbf{Description} \\ \hline
1 & ANA & O & Analog output \\ \hline
2 & REF & I & Reference voltage \\ \hline
3 & GND & O & Ground \\ \hline
4 & OUT & O & Binary (switching) output \\ \hline
5 & GND & O & Ground \\ \hline
6 & VCC & I & Operating voltage \\ \hline
7 & LDR & O & Optical resistance \\ \hline
8 & LDR & I & Optical resistance \\ \hline
\end{tabular}
\end{center}
\caption{Pin layout of PIR STD.\cite{PIROperationalManual}\label{fig:pirstdpin}}
\end{table}

The {\it PIR STD} scheme shown in the figure \ref{fig:pirstdscheme} processes the signal
in three stages going from left to right. The first two stages filter and amplify the signal,
output of this is the sensor analog output. The third phase generates binary output from the analog.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.75\textwidth]{img/pirstd.png}
\caption{Scheme of PIR STD.\cite{PIROperationalManual}\label{fig:pirstdscheme}}
\end{center}
\end{figure}

\paragraph{I. stage}
The first stage starts at S of the PIR sensor, following with noise/lowpass filter consisting
of the amplifier $U1A$ and the feedback components $R3$, $C4$, $C8$, $R14$, $C9$, $R14$.
There is also highpass filter done by $R6$ and $C3$. The output of this stage is a signal
with frequency between $f_{L1}$ and $f_{H1}$ amplified $A_{U1A}$ times.

\begin{subequations}
\begin{equation}
f_{H1} = \frac{1}{2 \pi R_{3,14} C_{4,8}}
\end{equation}

\begin{equation}
f_{H1}^{'} = \frac{1}{2 \pi R_{3,15} C_{4,9}}
\end{equation}
\end{subequations}

The choice of resistor $R_{3,14} / R_{3,15}$ and capacitator $C_{4,8} / C_{4,9}$ is done by
connected switch. The value of resistance and capacitance of the components can be counted
with formula for parallel connection.

\begin{equation}
R_{A,B}^{p} = \frac{1}{R_A} + \frac{1}{R_B}
\end{equation}

\begin{equation}
C_{A,B}^{p} = C_A + C_B
\end{equation}

The same for the resistors $R_{5,16}$ and $R_{5,17}$ and capacitators $C_{6,10}$ and $C_{6,11}$.

\begin{equation}
f_{L1} = \frac{1}{2 \pi R_6 C_3}
\end{equation}

\begin{subequations}
\begin{equation}
A_{U1A} = 1 + \frac{R_{3,14}}{R_2}
\end{equation}
\begin{equation}
A_{U1A}' = 1 + \frac{R_{3,15}}{R_2}
\end{equation}
\end{subequations}

\paragraph{II. stage}
The second processing stage focuses on amplification. It also includes lowpass filtering done
by $C5$ and $R4$ and highpass filter performed by the feedback of amplifier $U1B$. The greater
amplification is also made by the divider bridge ($R8$, $R9$, $R10$, $R11$) connected to the
positive input. The output of frequency between $max(f_{L1}, f_{L2})$ and $min(f_{H1}, f_{H2})$
amplified $A_{U1A} \cdot A_{U1B}$ times is an analog output connected to the pin 1.

\begin{subequations}
\begin{equation}
f_{H2} = \frac{1}{2 \pi R_{5,16} C_{6,10}}
\end{equation}

\begin{equation}
f_{H2}^{'} = \frac{1}{2 \pi R_{5,17} C_{6,11}}
\end{equation}
\end{subequations}

\begin{equation}
f_{L2} = \frac{1}{2 \pi R_4 C_5}
\end{equation}

\begin{subequations}
\begin{equation}
A_{U1B} = 1 + \frac{R_{5,16}}{R_4}
\end{equation}
\begin{equation}
A_{U1B}' = 1 + \frac{R_{5,17}}{R_4}
\end{equation}
\end{subequations}

\paragraph{III. stage}
The third phase performs top-bottom tresholding generating binary output
used in simple industrial application. It is not used in the project.\cite{PIRSchemeDescription}


\subsection*{Connecting the sensor}
PIR sensor is connected to the MCU. Except for source, ground and output which are connected directly,
{\it PIR STD} has also reference voltage input, that should be approximately $\frac{V_{cc}}{2}~V$.
To ensure that a voltage divider is used with two resistors of the same resistance $R_X$.
During the testing $100~k\Omega$ resistors were used. The circuit is shown in the figure \ref{fig:PIRcircuit}.

\begin{figure}[h!]
\begin{center}
\begin{circuitikz}
\ctikzset{bipoles/generic/height=0.2}
\ctikzset { label/align = straight }
\draw %(2,0)

  %to[V=$V_{Th}$] (0,2)
  %to[R=$R_{Th}$] (2.5,2)
  %to[short,i=$I$, -o] (4,2)
  %to[short] (4.5,2)
  % ref
  (0,2) to[short, l={\tiny $REF$}, o-]
  (1,2) to[short]
  (4,2) to[R={\tiny $R_{X}$}]
  (6,2) to[short,-*] (6,1.5)

  (2,0.5) to[short,*-]
  (2,2.5) to[R={\tiny $R_{X}$}]
  (4,2.5) to[short,-*] (4,2) 
  
  % vcc
  (0,1.5) to[short, l={\tiny $VCC$}, o-]
  (1,1.5) to[short]
  (7,1.5) to[short, l={\tiny $5~V$}, -o] (8,1.5)
  % signal
  (0,1) to[short, l={\tiny $ANA$}, o-] 
  (1,1) to[short]
  (7,1) to[short, l={\tiny $A1$}, -o] (8,1)
  % gnd
  (0,0.5) to[short, l={\tiny $GND$}, o-]
  (1,0.5) to[short]
  (7,0.5) to[short, l={\tiny $GND$}, -o] (8,0.5)

;
\node[draw,minimum width=2.5cm,minimum height=2.5cm,anchor=south east] at (0,0){PIR};
\node[draw,minimum width=2.5cm,minimum height=2.5cm,anchor=south west] at (8,0){MCU};
\end{circuitikz}
\caption{Connection of PIR and MCU.\label{fig:PIRcircuit}}
\end{center}
\end{figure}

\subsection*{Product design}
A printed circuit board was designed (figure \ref{fig:circuitboard}) and a prototype
was printed with a PCB drill in electronic laboratory at {\it FH Vorarlberg} to
illustrate the possible size and design of the final product.

The dimensions of final product were minimized to $50 \times 50 \times 25$~mm.
It was possible to prototype a board connecting the NodeMCU microcontroller,
sensor PIR STD, 2AA battery case and control elements (reset button, status LED)
within these dimensions. The sensor case design was briefly sketched too.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.3\textwidth]{render/circuitboard.png}
\caption{Circuit board. \label{fig:circuitboard}}
\end{center}
\end{figure}


\subsection*{Sampling}
The module is programmed to read signal in sampling frequency and send the data to server.

\paragraph{AD conversion}
The projection of voltage to a value is done by the built-in functionality, accessible by
standard library function \texttt{analogRead()}. The sets of analog values $A$ and digital values $D$
according to documentation of the function\cite{ArduinoAnalogRead} and the sensor\cite{PIROperationalManual}
and the morphism $c$ are shown in the formula \ref{eq:ad_projection}.

\begin{subequations}
\begin{equation}
A = \langle 0;V_{cc}\rangle
\end{equation}
\begin{equation}
D = \{0; 1; ..., 1023\}
\end{equation}
\begin{equation}
c: A \rightarrow D = x_A \rightarrow \frac{1023x_A}{V_{cc}}, x_A \in D
\end{equation}
\label{eq:ad_projection}
\end{subequations}

\paragraph{Sampling frequency}
The usable sampling frequency can be estimated: the fresnel lens of {\it PIR STD} splits the
area into $10^{\circ} = \frac{\pi}{18}$ circular sectors. Object moving around the sensor in the distance $0.5~m$
with speed $15~km.h^{-1} = 4.1667~m.s^{-1}$ (very fast run) passes the central circular sector in

\begin{equation}
t = \frac{s}{v} = \frac{0.5*tg(10^{\circ})}{4.1667} = 0.02116~s
\end{equation}

This means the frequency of the movement through the circular sectors is

\begin{equation}
f = \frac{1}{t} = \frac{1}{0.02116} = 47.259~Hz
\end{equation}

According to Shannon theorem\cite{DigitalSignalProcessing}, the sampling frequency must be at least twice as big as the
maximal frequency in the signal, which leads to

\begin{equation}
Fs \geq 2*47.259 = 94.518 
\end{equation}

Rounding up gives us minimal sampling frequency $100~Hz$, or sampling period $10~ms$.
resulting with $2B$ sample in throughput $\mu$

\begin{equation}
\mu = F_s * |\text{sample}| * 8\frac{bit}{byte} = 100 * 2 * 8 = 1.6~kbps
\end{equation}


\subsection*{MCU Program}
The NodeMCU is programmed to sample data with fixed sampling frequency and form the sequential
segments out of it. A $N$-sized segment is then sent away using ESP8266 present at the module.
A multicast technology is used, enabling multiple servers to work over the data concurrently
and also ease of initialization of communication, where the channel is predefined, so
no mutual IP address is needed. The sending frequency can be derived from sampling frequency $F_s$
with formula \ref{eq:sendfrequency}.

\begin{equation}
F_{send} = \frac{F_s}{N}
\label{eq:sendfrequency}
\end{equation}

The program is written in C++ and programming the module is done with Arduino IDE. Several libraries
are used that need to be downloaded to the IDE from its repository. ESP8266 libraries for networking
(multicast, HTTP) and EEPROM library to operate EEPROM persistent memory.

MCU runs HTTP server, enabling remote configuration of the MCU at the runtime without reprogramming
and reset. It is possible to set sampling frequency or period with the altering other correspondingly
as same as sending period, frequency, or segment size with a preset implementation limit.
The REST API also enables to set the multicast channel, both address and port, turning
debugging informations on/off to serial line.

\subsection*{REST API of MCU HTTP server}
The MCU runs a REST API, that can configure it. The API includes following options:

\begin{table}[h!]
    \begin{tabular}{|l|l|} \hline
        \textbf{Resource} & \textbf{Description} \\ \hline
        \texttt{/} & Welcome page. \\ \hline
        \texttt{/config} & Configuration of module. \\ \hline
        \texttt{/config/mcast}  & Configuration of multicast channel. \\ \hline
        \texttt{/config/sample} & Configuration of sampling. \\ \hline
        \texttt{/config/send}   & Configuration of sending. \\ \hline
        \texttt{/log}         & Logs. \\ \hline
    \end{tabular}
    \caption{REST API resources overview.}
\end{table}

\paragraph{\texttt{/config/mcast}}
Configures the multicast channel.
\begin{itemize}
    \item[] \texttt{enabled} Enables/disables sending. Defaultly enabled.
    \item[] \texttt{address} Sets multicast channel address. Defaultly \texttt{224.0.0.1}.
    \item[] \texttt{port}    Sets multicast channel port. Defaultly \texttt{1234}.
\end{itemize}

\paragraph{\texttt{/config/sample}}
Configures the sampling.
\begin{itemize}
    \item[] Exactly one of the parameters must be present (ignored otherwise):
    \item[] \texttt{frequency} Sets sampling frequency, subsequently changing sampling period (\ref{eq:period_frequency}) and segment size (\ref{eq:segmentsize}). Defaultly $100$.
    \item[] \texttt{period}    Sets sampling period, subsequently changing sampling frequency (\ref{eq:period_frequency}) and segment size (\ref{eq:segmentsize}). Defaultly $0.01$.
\end{itemize}

\paragraph{\texttt{/config/send}}
Configures the sending.
\begin{itemize}
    \item[] Exactly one of the parameters must be present (ignored otherwise):
    \item[] \texttt{frequency} Sets sampling frequency, subsequently changing sampling period (\ref{eq:period_frequency}) and segment size (\ref{eq:segmentsize}). Defaultly $0.5$.
    \item[] \texttt{period}    Sets sampling period, subsequently changing sampling frequency (\ref{eq:period_frequency}) and segment size (\ref{eq:segmentsize}). Defaultly $2$.
    \item[] \texttt{N}         Sets segment size, subsequently changing sampling frequency and period (\ref{eq:period_frequency}). Defaultly $200$.
\end{itemize}

The segment size must lay between 10 and 1024. If set value causes invalid value of segment size,
request will be ignored. The conversion to segment size $N$ is shown in the formula \ref{eq:segmentsize}.

\begin{subequations}
\begin{equation}
f = T^{-1}
\end{equation}
\begin{equation}
T = f^{-1}
\end{equation}
%\caption{Conversion period $T$ to frequency $f$ and vice versa.}
\label{eq:period_frequency}
\end{subequations}

\begin{subequations}
\begin{equation}
N = T_{send}T_s^{-1}
\end{equation}
\begin{equation}
N = f_sf_{send}^{-1}
\end{equation}
\begin{equation}
N = f_sT_{send}
\end{equation}
\begin{equation}
N = (T_sf_{send})^{-1}
\end{equation}
%\caption{Computation of segment size $N$.}
\label{eq:segmentsize}
\end{subequations}

\paragraph{\texttt{/log}}
Shows last events of the sensor in a form of logs.


\section{Classification server}
\label{chapter:serverImplementation}
{\it Monitor} is the classification server, that collects data from the sensors and performs
classification algorithms described in \ref{section:classification} with it.

The main software design is based on MVC (model-view-controller). The model reads the data from
file (offline), or right from serial line or multicast channel (online, realtime) and performs
a classification over them. The view is presenting the output of the model calculation to the user.

\subsection*{Data sources}
Monitor can read data from serial port, multicast channel or replay saved data from a file.
This feature is done using class \texttt{Reader} from the module \texttt{comm} and inherited classes
in modules \texttt{comm\_serial}, \texttt{comm\_mcast}, \texttt{comm\_replay}.

Class Reader corresponds with the design pattern Singleton, holding one single instance for each
serial port, multicast channel and file to avoid opening multiple handles and potential data race.

Instantiation of the object is done in the first demand for it in the call of static method \texttt{getReader()}.
Each object then possesses separate reading thread, that ensures updating of the data. Getting the last
received segment is through the method \texttt{getSegment()}, a thread lock is used. The whole design is shown
in the figure \ref{fig:class_src}.

The data source objects, inherited from \texttt{Reader} are assigned to the objects in view part. These graphical
objects hold reference to the \texttt{getSegment()} method of linked object in model part for access to the updated
data and to present them to the user.

The example of reading data using the \texttt{Reader} interface and its inherited classes is shown at the
listings \ref{listing:readFile} (offline) and  \ref{listing:readSerial} and \ref{listing:readMCast} (online).
When the data are read offline from file, the data are read as one chunk per file and processed together,
the online reading is performed in loop waiting for chunk to come from the sensor module.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/class_src.png}
\caption{UML diagram of Reader classes. \label{fig:class_src}}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{lstlisting}[style=python]
import comm_replay
# ...
x = comm_replay.Reader.readFile(path+'/'+filename+'.csv')
# ...
\end{lstlisting}
\caption{Reading file using \texttt{comm\_replay.py} module.\label{listing:readFile}}
\end{figure}

\begin{figure}[!ht]
\begin{lstlisting}[style=python]
import comm_serial
# ...
reader = comm_serial.Reader.getReader('/dev/ttyS0')
while True:
  x = reader.getSegment()
  # ...
\end{lstlisting}
\caption{Reading serial file using \texttt{comm\_serial.py} module.\label{listing:readSerial}}
\end{figure}

\begin{figure}[!ht]
\begin{lstlisting}[style=python]
import comm_mcast
# ...
reader = comm_mcast.Reader.getReader('224.0.0.1',1234)
while True:
  x = reader.getSegment()
  # ...
\end{lstlisting}
\caption{Reading multicast channel using \texttt{comm\_mcast.py} module.\label{listing:readMCast}}
\end{figure}



\subsection*{Classification model}
The segmentation and parsing into artefacts, which cover the feature extraction, is held by the module \texttt{segment.py}
with class diagram \ref{fig:class_segment}. The \texttt{parseArtefacts()} in this case is a factory for artefact vector.
In implementation it uses both \texttt{Segment} and \texttt{Edge}, these classes can be even used separately,
but the recommended way how to perform the feature extraction using this module is presented at the listing
\ref{listing:featureExtraction}.


\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.5\textwidth]{img/class_segment.png}
\caption{UML diagram of Segment classes. \label{fig:class_segment}}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{lstlisting}[style=python]
import segment
# ...
artefactsVector = segment.Artefact.parseArtefacts(x)
featuresVectorVector = [a.getFeatures() for a in artefactsVector]
# ...
\end{lstlisting}
\caption{Feature extraction using \texttt{segment.py} module.\label{listing:featureExtraction}}
\end{figure}


The each of the feature vectors is then separate input to the classifier, represented by the module
\texttt{model.py}. The recommended usage is shown at the listing \ref{listing:classification}

\begin{figure}[!ht]
\begin{lstlisting}[style=python]
import model
# ...
classifiers = model.Classification.getTrained()
for featuresVector in featuresVectorVector:
  areaMatrix = classifiers.evaluate()
  # ...
\end{lstlisting}
\caption{Classification using \texttt{model.py} module.\label{listing:classification}}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.8\textwidth]{img/class_model.png}
\caption{UML diagram of Model classes. \label{fig:class_model}}
\end{center}
\end{figure}

The output of each classifier was postprocessed in order to maximize the possible score. The evaluation
metrics is described in the section \ref{section:evaluationMetrics}. The postprocessing is doing over each
received chunk separately.

Operation, that is often used is using single smoothening filter with a memory,
defined formally by formula \ref{eq:smoothening}, filtering input artefact vector $a$ to
output $f(a)$, using inner memory vector $m$ and saturation function $S(x)$

$p$ is smoothening parameter with optimal value approximately $p \in (0,-1\rangle$,
where 0 means no influence and $-1$ means absolute influence of previous artefact.

\begin{subequations}
\begin{equation}
S(x) =\begin{cases} 1 & x > 1           \\
                    0 & x < 0           \\
                    x & \text{otherwise}
      \end{cases}
\end{equation}
\begin{equation}
m[0] = 0
\end{equation}
\begin{equation}
f_{p}(A) = \forall i \in \langle 0;|A|):  f(a[i]) = S(A[i] + m[i]), m[i+1] = S(S(A[i] + m[i]) + |A[i]|*p)
\end{equation}
\label{eq:smoothening}
\end{subequations}

To find the best fitting parameter $p$ value, equation \ref{eq:smoothening} can be
altered in order to run the optimalization by maximizing conjunction with vector of
reference keys $\kappa_{X}$ for each trait over the whole train set $\forall A$,
as shown in the formula \ref{eq:smootheningOptimalization}.
This operation is introduced later in the table \ref{table:3stateLogicOp}.

\begin{equation}
argmax_{p} (\sum_{\forall A} f_{p}(A)\cdot \kappa_{X}(A) )* 100\%
\label{eq:smootheningOptimalization}
\end{equation}


%\section{Collector} 
%{\it Collector} is a program, as the name says, that collects data from sensors and implements
%the whole described algorithm. Classified objects are then sent to a {\it visualizer}.

%\subsection*{Communication}
%The communication of the sensor module and the client app is designed as a client-server. The results
%from the classification performed by server are sent to the client which shows it to user.

%As the technology for the channel a LAN multicast stream is used at 224.0.0.128:12345.
%The main advantage in comparison to unicast is a support of multiple clients.

%\section{Visualizer}
%Client app called {\it Visualizer} is written in {\it Python3} and graphical library {\it Tkinter}.
%This choice was made with portability of the program taken into consideration.
%The design of user interface is described in chapter \ref{Label:UI}.




\chapter{Experiments}

\section{Evaluation}
\label{section:evaluationMetrics}
A several metrics can be used to evaluate the success rate of decision.
In following equations, $A$ stands for tuple of $|A|$ artefacts being
evaluated, $A[i]$ for an artefact from this set. For each artefact from testing
set, a label $\kappa_{X}(A[i])$ for all traits is known.

For labeling purposes, a variation of Lukasiewicz logic was used to cover
the states, which Bool logic is not able to handle, as the position of person
in case area is empty. The operation in the table \ref{table:3stateLogicOp} is
represents comparing the computed score and the real label returning a score
of success of the decision.

Input conditions for fuzzy number $a \in \langle 0;1 \rangle$
and $\kappa \in \{ 1, 0, N\}$, which is in {\it json} file format represented
by {\it True}, {\it False} and {\it None} literals respectively. The output
is a success rate, multiplied by $100$ can be converted to percentage.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|} \hline
\textbf{a}  & \textbf{$\kappa$} & \textbf{$a \cdot \kappa$} \\ \hline
$a$         & 1               & $a$                       \\ \hline
$a$         & 0               & $N(a)$                    \\ \hline
$a$         & N               & \it{excluded}             \\ \hline
\end{tabular}
\caption{Classifier output comparing to reference.\label{table:3stateLogicOp} }
\end{center}
\end{table}

To emulate the decision by tresholding a fuzzy sigmoid can be applied to $a$,
mapping fuzzy values to fuzzy values, which are more decisive. An example is
shown at the equation \ref{eq:fuzzySigmoid}. The usage and choice of such
function distorts results significantly.

\begin{equation}
\sigma(a) = \begin{cases} 1 & a \geq 0.5 \\
            0 & \text{otherwise} \end{cases}
\label{eq:fuzzySigmoid}
\end{equation}

The metrics used and described below use also notation described by the equations
\ref{eq:tupleArtefactNotation}. These tuples contains only artefacts, that are
positive for the trait $X$ ($A_{X}^{1}$), or negative ($A_{X}^{0}$).

\begin{subequations}
\begin{equation}
A_{X}^{1} = (\forall a \in A: \kappa_{X}(a) = 1)
\end{equation}
\begin{equation}
A_{X}^{0} = (\forall a \in A \kappa_{X}(a) = 0)
\end{equation}
\label{eq:tupleArtefactNotation}
\end{subequations}

First possible metric is a simple average of artefacts with its scores by the
formula \ref{eq:artefactAverage} for trait $X$. Similarly can be counted
separately success rate for $A_{X}^{1}$ and $A_{X}^{0}$.

\begin{equation}
\overline{A_{X}} = \frac{1}{|A|} \sum_{a\in A} a_{X} \cdot \kappa_{X}(a)
\label{eq:artefactAverage}
\end{equation}

The metric \ref{eq:artefactAverage} does not take account of artefact length
as a weight. To do so, alternation \ref{eq:artefactWeightedAverage} with the
weight added is suggested.

\begin{equation}
\overline{A_{X}}' = \frac{1}{\sum_{A}|a|}\sum_{a\in A} (a_{X} \cdot \kappa_{X}(a)) |a|
\label{eq:artefactWeightedAverage}
\end{equation}

\section{Smoothing parameters optimalization}

Optimalization of parameters in postprocessing smoothing function was performed
according to the \ref{eq:smootheningOptimalization}. The parameters for each trait
were computed, as shown by the table \ref{table:smootheningParameters}.

\begin{table}[!ht]
\begin{center}
\begin{tabular}{|c|c c c c|} \hline
\textbf{Direction}  & \textbf{Presence} & \textbf{Distance}*& \textbf{Center} & \textbf{Left} \\ \hline
Forwards            & $-0.085$          & $-0.001$          & $-0.005$        & N             \\
Backwards           & $-0.0025$         & $-0.1$            & $-0.01$         & N             \\ \hline
\end{tabular}
\caption{Experimentally computed smoothening parameters values.\label{table:smootheningParameters} }
\end{center}
\end{table}

\section{Results}

The table \ref{table:results} includes certainty of decision to presence or absence of the trait,
equal to $100\overline{A_{X}}'$. In the case of presence, positive means presence of body in area,
negative means absence, for distance it is body being further than $4.5~m$ when positive, closer when negative.
Center is positive for body being in front of the sensor, negative for body on the side.

Since the training is done by numerical solving of gradient descent, every training results in different
result. To have more reflective results, accuracy was measured as mean of results of 10 successive trainings
and evaluating of the test data set for each trait.

The parts of data set (4m\_* and 5m\_*) were not used due to the lack of other more distinct data especially in
the distance trait (for example 8m\_*) which significantly distorts the negative rate.

\begin{table}[!ht]
\begin{center}
\begin{tabular}{|c|c c c c|} \hline
\textbf{Aspect} & \textbf{Presence} & \textbf{Distance} & \textbf{Center} & \textbf{Left} \\ \hline
Positive rate   & $75.491$          & $74.266$          & $61.866$              & $52.540$       \\
Negative rate   & $87.083$          & $70.681$          & $52.823$              & $53.451$       \\ \hline
Total rate      & $83.743$          & $$                & $$                    & $$             \\ \hline
\end{tabular}
\caption{Resulting accurracy on testing data.\label{table:results} }
\end{center}
\end{table}

Reading the table \ref{table:results} as probability would be miscomprehension, to achieve the posterior probability for
these traits, the outputs must be converted from fuzzy to bool logic (e.g. tresholding) and the formula \ref{eq:posteriorProb}
to compute the resulting posterior probability is used over them. $T(x) \rightarrow {0,1}$ is tresholding function mapping
fuzzy values to bool values with treshold 0.5.

\begin{equation}
p(\kappa \in {0,1}|A)_{X} = \frac{1}{\sum_{A^{\kappa}}|a|} \sum_{a\in A^{\kappa}} (T(a_{X}))|a|
\label{eq:posteriorProb}
\end{equation}

\begin{table}[!ht]
\begin{center}
\begin{tabular}{|c|c c c c|} \hline
\textbf{Aspect} & \textbf{Presence} & \textbf{Distance} & \textbf{Center} & \textbf{Left} \\ \hline
Positive rate   & $75.244$          & $$                & $$              & $$            \\
Negative rate   & $86.539$          & $$                & $$              & $$            \\ \hline
Total rate      & $$                & $$                & $$              & $$            \\ \hline
\end{tabular}
\caption{Posterior probability on testing data.\label{table:resultsProbability} }
\end{center}
\end{table}

The results of posterior probability, shown in the table \ref{table:resultsProbability}, show,
that it is definitely possible to use PIR sensors for the purpose of localization of people
and possibly even counting them. The results itself in the tables \ref{table:results} and
\ref{table:resultsProbability} are probably not sufficient for a deployment in the real situations.
The relatively low results can be caused by multiple aspects.

The most obvious aspect that could influence the accuracy of the classification of the testing data
is the method of labeling. The reference medium for labeling were video recordings, synchronized
with the recording session in the monitor by a Monitor generating a sound at the beginning of each
recording and then synchronisation using the sound track of the video to mark and crop the recording.

Another aspect causing unfavourable result values could be labeling itself. It was done manually,
so the human aspect is one of the reasons as same as the fact, that labels were created for each
splitted artefact itself. Potential mistakes in segmenting and merging the segments into artefacts
could be propagated into the results, even though there was an effort to reduce this threat in
class Reference, which performs operations across the artefacts.

Not mentioned, though quite important is a size of dataset. During the whole development, about 200
15~s recordings were done, each then splitted into 10 - 30 artefacts, which each possess a separate
label. For the training and testing purposes only 81 recordings were later included, 45 with no
movement (empty area) and 36 included movement.


\subsection*{Suggestions for improvement}
As mentioned before, the results are not very positive for the deployment in the real situation, e.g.
controlling of objects etc. With further improvements and reducing of the mentioned risks it could
be a very good and promising principle for certain fields, as physical security, internet of things
and others.

The improvement is necessary to reach it. Firstly, more data and different movement types must be
recorded in order to improve both the accuracy and possibility to use the sensor in general. 
The possible way could be changing not only the orientations and trajectories of the movement,
but also the speed, different surroundings, sizes of the area, temperature differences in the
background (usage in sauna or a room freezers).

The multiple persons present, even though mechanism considered possibility of counting the people,
were not tested or measured at all during the measuring.

The speed of the processing program is not breakneck. Possible way could be profiling the program and optimizing
the critical parts by rewriting them in compiled language (C++). Since the program includes a lot of vector
algebra, optimalization with moving the computation to GPU / SSE / AVX could be also a solution.
During the offline processing, parallelism could be a good approach to increase a speed of processing.

Multiple sensors do not only have to be used in one single room being merged by fusion, but
another improvement could be designing a server, monitoring and identification of the people by
a network of PIR sensors and cameras. Such a mechanism was already matter of research in
several theses, such as \cite{KenyaThesis}.




\chapter{User Interface}
\label{Label:UI}

{\it Monitor} as a classification server is endowed with simple GUI for the needs of monitoring
the signal real-time and recording of the data sets used for training, testing and evaluating the results.
The interface is shown in the figure \ref{fig:monitorGUI}.

Since the programming language used for the Monitor was Python3 due to many amazing libraries focused on the
machine learning and classification (numpy, scipy, matplotlib), for GUI was chosen a library {\it TKinter}
-- as a part of standard Python3 library is always present whenever Python3 is, the usage is pretty simple
and it is easy to connect with matplotlib, graph plotting library. The whole GUI is still designed as a prototype,
for the system deployed to the real situations it would be probably too slow and not flexible enough.
As an alternative, Qt might be considered.

\subsection*{Structure of the view}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.7\textwidth]{render/gui.png}
\caption{Monitor GUI prototype with sample data. \label{fig:monitorGUI}}
\end{center}
\end{figure}

The classical menu bar with dropdowns appears in the top of the window. Dropdown {\it File} contains
option to switch to the record mode over the source in the active tab and opening the tab with a
source from file.
The side menu contains possible sources to read: multicast channels or serial ports. Checking
opens a new tab with a real-time view of the signal of the source as well as the area view,
classified by the system. 

In order to record a signal into a file, a recording session needs to be created and arranged.
The session is made from recordings. The lengths of recordings and time delays before each recording
are adjustable. This enables recording session to fulfil the needs for any kind of movement to be made.
To work properly with the program, the names of the exercises must match the name of the session and they
must have appendixes \_1, \_2, ... . An example is shown in the figure \ref{fig:monitorRecordGUI}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.2\textwidth]{render/record.png}
\caption{Monitor GUI recording session dialog.\label{fig:monitorRecordGUI}}
\end{center}
\end{figure}

\subsection*{Structure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=1\textwidth]{img/class_gui.png}
\caption{Class diagram of view part. \label{fig:classGUI}}
\end{center}
\end{figure}

The whole Monitor is made as model-view-controller pattern. The view consists of module {\it app.py} with
the class App controlling the processes inside of the view. Two more modules are present, {\it ui.py}
and {\it views.py}.

The {\it ui.py} is a adapter over several TKinter classes with slight extension of its functionality according
to the needs of App class. It contains CheckButton connected with a data source, which repeats testing the 
connected source availability and enables or disables itself by that. Notebook extends {\it TKinter.ttk.Notebook}
with a simple graphical widget, Menu represents the menubar.

The whole graphical design consists of multiple parts with various roles, assembled together in one window.
For scalability reasons each of this "views" is implemented as a separate class inside the module {\it views.py}.
Classes MulticastView and SerialView are used in the side menus with checkbuttons for each data source, multicast
channel or serial port. SignalView and AreaView are the main part of the tab system (notebook), visualizing the
received segment and the counted area fuzzy matrix. The ReplayView can be seen when a replay is shown as a data
source, it contains the button controlling the replaying of the file.






\chapter{Conclusion}

Nowadays PIR sensors are mostly used to detect a presence of a person. The main aim of the research was to try to use PIR sensors
for monitoring the situation in the sensed area and state the count of people. For this matter a system, that would use the data
from the PIR sensors and scan the monitor situation, should have been designed. To do so, various classification and recognition
algorithms were studied and considered. Suggested solutions by the task were using either predefined fuzzy logic system or
artificial learning system. One of the tasks was also to implement suggested algorithm and verify its functionality on real situations.

The issues related to the topic, PIR sensor and MCU as for hardware and classification algorithms and fuzzy logic in particular, were
studied in detail. To understand the area even more thoroughly, a number of theses and articles were read. The approach of evaluating
the room occupancy using infrared radiation is still new and seems very promising.

Acquired data enabled designing a theoretical system based on fuzzy logic and linear regression for classification itself. The system
could be used not only to state either presence or absence of a person in front of the sensor, but also his position and possibly
even the count of people, if multiple are present. The design also suggests a way how to use multiple sensors.

The implementation of the design includes the main processing pipeline for one sensor, where the signal can be either pre-recorded
and read from a file, or sensed in real time and transferred by serial port, or by multicast group. For monitoring and debugging
purposes, a prototype of GUI was made, showing the real time signal and the processing output.

The PCB of a possible product, a sensing device, and the look of it, was suggested as well. A prototype of PCB was drilled.

The classifiers were trained with supervisor. To do so, a train and test data set wer recorded and labelled with simple labeling method,
and experiments were done over it. The specified formal points of the task were all fulfilled.

The designed system considers much wider usage, than was implemented, as multiple-sensor usage and the fusion algorithm. Multiple-person
situations were not tested. Still the usage of PIR sensors in this application has bright prospects and despite the challenges, that
are still in the way, it is definitely worth being focus for onward research.






%===============================================================================
